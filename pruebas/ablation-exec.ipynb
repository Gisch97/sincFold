{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch as tr\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import torch.nn as nn\n",
    "from sincfold.dataset import SeqDataset, pad_batch\n",
    "from torch.utils.data import DataLoader\n",
    "from sincfold.model import sincfold\n",
    "\n",
    "from sincfold.ablation.ablation_1ResNet2d import sincfold_1ResNet2d\n",
    "from sincfold.ablation.ablation_no_ResNet1d_FF import sincfold_no_ResNet1d_FF\n",
    "from sincfold.ablation.ablation_no_ResNet2d import sincfold_no_ResNet2d\n",
    "from sincfold.ablation.ablation_C1D_C2D import sincfold_C1D_C2D\n",
    "from sincfold.ablation.ablation_no_ResNet1d import sincfold_no_ResNet1d\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "tr.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Params\n",
    "train_file ='../data/ArchiveII_sample.csv'\n",
    "config={}\n",
    "valid_file=None\n",
    "nworkers=2\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sincfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ./pruebas/original\n",
      "No weights provided, using random initialization\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [07:25<00:00, 148.48s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,train_f1,train_loss,valid_f1,valid_f1_post,valid_loss\n",
      "\n",
      "0,0.0016,0.5476,0.0000,0.0000,0.1929\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:42<00:00, 114.01s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0.0000,0.2032,0.0000,0.0000,0.2465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:34<00:00, 111.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,0.0000,0.2356,0.0000,0.0000,0.2406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:06<00:00, 122.31s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,0.0000,0.1556,0.0000,0.0000,0.2045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:00<00:00, 120.27s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,0.0000,0.1079,0.0000,0.0000,0.1705\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:08<00:00, 102.84s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,0.0000,0.0802,0.0000,0.0000,0.1437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out_path='./pruebas/original'\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print(\"Working on\", out_path)\n",
    "\n",
    "if \"cache_path\" not in config:\n",
    "    config[\"cache_path\"] = \"cache/\"\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.makedirs(out_path)\n",
    "else:\n",
    "    raise ValueError(f\"Output path {out_path} already exists\")\n",
    "\n",
    "if valid_file is not None:\n",
    "    train_file = train_file\n",
    "    valid_file = valid_file\n",
    "else:\n",
    "    data = pd.read_csv(train_file)\n",
    "    valid_split = config[\"valid_split\"] if \"valid_split\" in config else 0.1\n",
    "    train_file = os.path.join(out_path, \"train.csv\")\n",
    "    valid_file = os.path.join(out_path, \"valid.csv\")\n",
    "\n",
    "    val_data = data.sample(frac = valid_split)\n",
    "    val_data.to_csv(valid_file, index=False)\n",
    "    data.drop(val_data.index).to_csv(train_file, index=False)\n",
    "    \n",
    "batch_size = config[\"batch_size\"] if \"batch_size\" in config else 4\n",
    "train_loader = DataLoader(\n",
    "    SeqDataset(train_file, training=True, **config),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    SeqDataset(valid_file, **config),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch,\n",
    ")\n",
    "\n",
    "net = sincfold(train_len=len(train_loader), **config)\n",
    "\n",
    "best_f1, patience_counter = -1, 0\n",
    "patience = config[\"patience\"] if \"patience\" in config else 30\n",
    "if verbose:\n",
    "    print(\"Start training...\")\n",
    "max_epochs = config[\"max_epochs\"] if \"max_epochs\" in config else 1000\n",
    "logfile = os.path.join(out_path, \"train_log.csv\") \n",
    "    \n",
    "for epoch in range(6): ## 6 epochs\n",
    "    train_metrics = net.fit(train_loader)\n",
    "\n",
    "    val_metrics = net.test(valid_loader)\n",
    "\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        tr.save(net.state_dict(), os.path.join(out_path, \"weights.pmt\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            break\n",
    "    \n",
    "    if not os.path.exists(logfile):\n",
    "        with open(logfile, \"w\") as f: \n",
    "            msg = ','.join(['epoch']+[f\"train_{k}\" for k in sorted(train_metrics.keys())]+[f\"valid_{k}\" for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "            f.write(msg)\n",
    "            f.flush()\n",
    "            if verbose:\n",
    "                print(msg)\n",
    "\n",
    "    with open(logfile, \"a\") as f: \n",
    "        msg = ','.join([str(epoch)]+[f'{train_metrics[k]:.4f}' for k in sorted(train_metrics.keys())]+[f'{val_metrics[k]:.4f}' for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "        f.write(msg)\n",
    "        f.flush()    \n",
    "        if verbose:\n",
    "            print(msg)\n",
    "        \n",
    "# remove temporal files           \n",
    "shutil.rmtree(config[\"cache_path\"], ignore_errors=True)\n",
    "\n",
    "tmp_file = os.path.join(out_path, \"train.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)\n",
    "tmp_file = os.path.join(out_path, \"valid.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ablation: 1 ResNet2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ./pruebas/1_ResNet2d/\n",
      "No weights provided, using random initialization\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:48<00:00, 96.26s/it] \n",
      "100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,train_f1,train_loss,valid_f1,valid_f1_post,valid_loss\n",
      "\n",
      "0,0.0002,0.1727,0.0000,0.0000,0.2271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:29<00:00, 89.92s/it] \n",
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0.0000,0.1579,0.0000,0.0000,0.2319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:11<00:00, 103.86s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,0.0000,0.1369,0.0000,0.0000,0.2004\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:30<00:00, 90.21s/it] \n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,0.0000,0.0961,0.0000,0.0000,0.1708\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:35<00:00, 91.90s/it] \n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,0.0198,0.0655,0.0556,0.0556,0.1565\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:45<00:00, 95.05s/it] \n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,0.1438,0.0544,0.2821,0.2105,0.1578\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_file ='../data/ArchiveII_sample.csv'\n",
    "\n",
    "valid_file=None\n",
    "out_path='./pruebas/1_ResNet2d/'\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print(\"Working on\", out_path)\n",
    "\n",
    "if \"cache_path\" not in config:\n",
    "    config[\"cache_path\"] = \"cache/\"\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.makedirs(out_path)\n",
    "else:\n",
    "    raise ValueError(f\"Output path {out_path} already exists\")\n",
    "\n",
    "if valid_file is not None:\n",
    "    train_file = train_file\n",
    "    valid_file = valid_file\n",
    "else:\n",
    "    data = pd.read_csv(train_file)\n",
    "    valid_split = config[\"valid_split\"] if \"valid_split\" in config else 0.1\n",
    "    train_file = os.path.join(out_path, \"train.csv\")\n",
    "    valid_file = os.path.join(out_path, \"valid.csv\")\n",
    "\n",
    "    val_data = data.sample(frac = valid_split)\n",
    "    val_data.to_csv(valid_file, index=False)\n",
    "    data.drop(val_data.index).to_csv(train_file, index=False)\n",
    "    \n",
    "batch_size = config[\"batch_size\"] if \"batch_size\" in config else 4\n",
    "train_loader = DataLoader(\n",
    "    SeqDataset(train_file, training=True, **config),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    SeqDataset(valid_file, **config),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch,\n",
    ")\n",
    "\n",
    "net = sincfold_1ResNet2d(train_len=len(train_loader), **config)\n",
    "\n",
    "best_f1, patience_counter = -1, 0\n",
    "patience = config[\"patience\"] if \"patience\" in config else 30\n",
    "if verbose:\n",
    "    print(\"Start training...\")\n",
    "max_epochs = config[\"max_epochs\"] if \"max_epochs\" in config else 1000\n",
    "logfile = os.path.join(out_path, \"train_log.csv\") \n",
    "    \n",
    "for epoch in range(6): ## 6 epochs\n",
    "    train_metrics = net.fit(train_loader)\n",
    "\n",
    "    val_metrics = net.test(valid_loader)\n",
    "\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        tr.save(net.state_dict(), os.path.join(out_path, \"weights.pmt\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            break\n",
    "    \n",
    "    if not os.path.exists(logfile):\n",
    "        with open(logfile, \"w\") as f: \n",
    "            msg = ','.join(['epoch']+[f\"train_{k}\" for k in sorted(train_metrics.keys())]+[f\"valid_{k}\" for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "            f.write(msg)\n",
    "            f.flush()\n",
    "            if verbose:\n",
    "                print(msg)\n",
    "\n",
    "    with open(logfile, \"a\") as f: \n",
    "        msg = ','.join([str(epoch)]+[f'{train_metrics[k]:.4f}' for k in sorted(train_metrics.keys())]+[f'{val_metrics[k]:.4f}' for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "        f.write(msg)\n",
    "        f.flush()    \n",
    "        if verbose:\n",
    "            print(msg)\n",
    "        \n",
    "# remove temporal files           \n",
    "shutil.rmtree(config[\"cache_path\"], ignore_errors=True)\n",
    "\n",
    "tmp_file = os.path.join(out_path, \"train.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)\n",
    "tmp_file = os.path.join(out_path, \"valid.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sincfold_no_ResNet1d_FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ./pruebas/no_ResNet1d_FF/\n",
      "No weights provided, using random initialization\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x356 and 4x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m logfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_log.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m): \u001b[38;5;66;03m## 6 epochs\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mtest(valid_loader)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m best_f1:\n",
      "File \u001b[0;32m~/miniconda3/envs/sincfold/lib/python3.9/site-packages/sincfold/ablation/ablation_no_ResNet1d_FF.py:245\u001b[0m, in \u001b[0;36mSincFold.fit\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    243\u001b[0m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontact\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Cleaning cache optimizer\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(y_pred, y)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# y_pred is a composed tensor, we need to get the final pred\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sincfold/lib/python3.9/site-packages/sincfold/ablation/ablation_no_ResNet1d_FF.py:172\u001b[0m, in \u001b[0;36mSincFold.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    169\u001b[0m L \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# y = self.resnet1d(x)\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m ya \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeedForward1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m ya \u001b[38;5;241m=\u001b[39m relu(ya)\n\u001b[1;32m    174\u001b[0m ya \u001b[38;5;241m=\u001b[39m tr\u001b[38;5;241m.\u001b[39mtranspose(ya, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x356 and 4x32)"
     ]
    }
   ],
   "source": [
    "train_file ='../data/ArchiveII_sample.csv'\n",
    "\n",
    "valid_file=None\n",
    "out_path='./pruebas/no_ResNet1d_FF/'\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print(\"Working on\", out_path)\n",
    "\n",
    "if \"cache_path\" not in config:\n",
    "    config[\"cache_path\"] = \"cache/\"\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.makedirs(out_path)\n",
    "else:\n",
    "    raise ValueError(f\"Output path {out_path} already exists\")\n",
    "\n",
    "if valid_file is not None:\n",
    "    train_file = train_file\n",
    "    valid_file = valid_file\n",
    "else:\n",
    "    data = pd.read_csv(train_file)\n",
    "    valid_split = config[\"valid_split\"] if \"valid_split\" in config else 0.1\n",
    "    train_file = os.path.join(out_path, \"train.csv\")\n",
    "    valid_file = os.path.join(out_path, \"valid.csv\")\n",
    "\n",
    "    val_data = data.sample(frac = valid_split)\n",
    "    val_data.to_csv(valid_file, index=False)\n",
    "    data.drop(val_data.index).to_csv(train_file, index=False)\n",
    "    \n",
    "batch_size = config[\"batch_size\"] if \"batch_size\" in config else 4\n",
    "train_loader = DataLoader(\n",
    "    SeqDataset(train_file, training=True, **config),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    SeqDataset(valid_file, **config),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch,\n",
    ")\n",
    "\n",
    "net = sincfold_no_ResNet1d_FF(train_len=len(train_loader), **config)\n",
    "\n",
    "best_f1, patience_counter = -1, 0\n",
    "patience = config[\"patience\"] if \"patience\" in config else 30\n",
    "if verbose:\n",
    "    print(\"Start training...\")\n",
    "max_epochs = config[\"max_epochs\"] if \"max_epochs\" in config else 1000\n",
    "logfile = os.path.join(out_path, \"train_log.csv\") \n",
    "    \n",
    "for epoch in range(6): ## 6 epochs\n",
    "    train_metrics = net.fit(train_loader)\n",
    "\n",
    "    val_metrics = net.test(valid_loader)\n",
    "\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        tr.save(net.state_dict(), os.path.join(out_path, \"weights.pmt\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            break\n",
    "    \n",
    "    if not os.path.exists(logfile):\n",
    "        with open(logfile, \"w\") as f: \n",
    "            msg = ','.join(['epoch']+[f\"train_{k}\" for k in sorted(train_metrics.keys())]+[f\"valid_{k}\" for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "            f.write(msg)\n",
    "            f.flush()\n",
    "            if verbose:\n",
    "                print(msg)\n",
    "\n",
    "    with open(logfile, \"a\") as f: \n",
    "        msg = ','.join([str(epoch)]+[f'{train_metrics[k]:.4f}' for k in sorted(train_metrics.keys())]+[f'{val_metrics[k]:.4f}' for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "        f.write(msg)\n",
    "        f.flush()    \n",
    "        if verbose:\n",
    "            print(msg)\n",
    "        \n",
    "# remove temporal files           \n",
    "shutil.rmtree(config[\"cache_path\"], ignore_errors=True)\n",
    "\n",
    "tmp_file = os.path.join(out_path, \"train.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)\n",
    "tmp_file = os.path.join(out_path, \"valid.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## sincfold_no_ResNet2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ./pruebas/no_ResNet2d/\n",
      "No weights provided, using random initialization\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:09<00:00, 123.17s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,train_f1,train_loss,valid_f1,valid_f1_post,valid_loss\n",
      "\n",
      "0,0.0022,0.5878,0.0000,0.0000,0.0897\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:31<00:00, 90.41s/it] \n",
      "100%|██████████| 1/1 [00:08<00:00,  8.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0.0000,0.2281,0.0000,0.0000,0.1161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:31<00:00, 130.42s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,0.0000,0.3316,0.0000,0.0000,0.1142\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [07:01<00:00, 140.55s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,0.0000,0.2432,0.0000,0.0000,0.0950\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:26<00:00, 128.90s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,0.0000,0.2407,0.0000,0.0000,0.0781\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:40<00:00, 113.51s/it]\n",
      "100%|██████████| 1/1 [00:13<00:00, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,0.0000,0.1362,0.0000,0.0000,0.0681\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_file ='../data/ArchiveII_sample.csv'\n",
    "\n",
    "valid_file=None\n",
    "out_path='./pruebas/no_ResNet2d/'\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print(\"Working on\", out_path)\n",
    "\n",
    "if \"cache_path\" not in config:\n",
    "    config[\"cache_path\"] = \"cache/\"\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.makedirs(out_path)\n",
    "else:\n",
    "    raise ValueError(f\"Output path {out_path} already exists\")\n",
    "\n",
    "if valid_file is not None:\n",
    "    train_file = train_file\n",
    "    valid_file = valid_file\n",
    "else:\n",
    "    data = pd.read_csv(train_file)\n",
    "    valid_split = config[\"valid_split\"] if \"valid_split\" in config else 0.1\n",
    "    train_file = os.path.join(out_path, \"train.csv\")\n",
    "    valid_file = os.path.join(out_path, \"valid.csv\")\n",
    "\n",
    "    val_data = data.sample(frac = valid_split)\n",
    "    val_data.to_csv(valid_file, index=False)\n",
    "    data.drop(val_data.index).to_csv(train_file, index=False)\n",
    "    \n",
    "batch_size = config[\"batch_size\"] if \"batch_size\" in config else 4\n",
    "train_loader = DataLoader(\n",
    "    SeqDataset(train_file, training=True, **config),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    SeqDataset(valid_file, **config),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch,\n",
    ")\n",
    "\n",
    "net = sincfold_no_ResNet2d(train_len=len(train_loader), **config)\n",
    "\n",
    "best_f1, patience_counter = -1, 0\n",
    "patience = config[\"patience\"] if \"patience\" in config else 30\n",
    "if verbose:\n",
    "    print(\"Start training...\")\n",
    "max_epochs = config[\"max_epochs\"] if \"max_epochs\" in config else 1000\n",
    "logfile = os.path.join(out_path, \"train_log.csv\") \n",
    "    \n",
    "for epoch in range(6): ## 6 epochs\n",
    "    train_metrics = net.fit(train_loader)\n",
    "\n",
    "    val_metrics = net.test(valid_loader)\n",
    "\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        tr.save(net.state_dict(), os.path.join(out_path, \"weights.pmt\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            break\n",
    "    \n",
    "    if not os.path.exists(logfile):\n",
    "        with open(logfile, \"w\") as f: \n",
    "            msg = ','.join(['epoch']+[f\"train_{k}\" for k in sorted(train_metrics.keys())]+[f\"valid_{k}\" for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "            f.write(msg)\n",
    "            f.flush()\n",
    "            if verbose:\n",
    "                print(msg)\n",
    "\n",
    "    with open(logfile, \"a\") as f: \n",
    "        msg = ','.join([str(epoch)]+[f'{train_metrics[k]:.4f}' for k in sorted(train_metrics.keys())]+[f'{val_metrics[k]:.4f}' for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "        f.write(msg)\n",
    "        f.flush()    \n",
    "        if verbose:\n",
    "            print(msg)\n",
    "        \n",
    "# remove temporal files           \n",
    "shutil.rmtree(config[\"cache_path\"], ignore_errors=True)\n",
    "\n",
    "tmp_file = os.path.join(out_path, \"train.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)\n",
    "tmp_file = os.path.join(out_path, \"valid.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## sincfold_C1D_C2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ./pruebas/C1D_C2D/\n",
      "No weights provided, using random initialization\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.64s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,train_f1,train_loss,valid_f1,valid_f1_post,valid_loss\n",
      "\n",
      "0,0.0148,0.8448,0.0000,0.0000,0.4140\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.84s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0.0014,0.3265,0.0000,0.0000,0.1648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.75s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,0.0000,0.1865,0.0000,0.0000,0.0991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.61s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,0.0000,0.1203,0.0000,0.0000,0.0839\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.46s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,0.0000,0.1768,0.0000,0.0000,0.0819\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.57s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,0.0000,0.1651,0.0000,0.0000,0.0826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_file ='../data/ArchiveII_sample.csv'\n",
    "\n",
    "valid_file=None\n",
    "out_path='./pruebas/C1D_C2D/'\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print(\"Working on\", out_path)\n",
    "\n",
    "if \"cache_path\" not in config:\n",
    "    config[\"cache_path\"] = \"cache/\"\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.makedirs(out_path)\n",
    "else:\n",
    "    raise ValueError(f\"Output path {out_path} already exists\")\n",
    "\n",
    "if valid_file is not None:\n",
    "    train_file = train_file\n",
    "    valid_file = valid_file\n",
    "else:\n",
    "    data = pd.read_csv(train_file)\n",
    "    valid_split = config[\"valid_split\"] if \"valid_split\" in config else 0.1\n",
    "    train_file = os.path.join(out_path, \"train.csv\")\n",
    "    valid_file = os.path.join(out_path, \"valid.csv\")\n",
    "\n",
    "    val_data = data.sample(frac = valid_split)\n",
    "    val_data.to_csv(valid_file, index=False)\n",
    "    data.drop(val_data.index).to_csv(train_file, index=False)\n",
    "    \n",
    "batch_size = config[\"batch_size\"] if \"batch_size\" in config else 4\n",
    "train_loader = DataLoader(\n",
    "    SeqDataset(train_file, training=True, **config),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    SeqDataset(valid_file, **config),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch,\n",
    ")\n",
    "\n",
    "net = sincfold_C1D_C2D(train_len=len(train_loader), **config)\n",
    "\n",
    "best_f1, patience_counter = -1, 0\n",
    "patience = config[\"patience\"] if \"patience\" in config else 30\n",
    "if verbose:\n",
    "    print(\"Start training...\")\n",
    "max_epochs = config[\"max_epochs\"] if \"max_epochs\" in config else 1000\n",
    "logfile = os.path.join(out_path, \"train_log.csv\") \n",
    "    \n",
    "for epoch in range(6): ## 6 epochs\n",
    "    train_metrics = net.fit(train_loader)\n",
    "\n",
    "    val_metrics = net.test(valid_loader)\n",
    "\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        tr.save(net.state_dict(), os.path.join(out_path, \"weights.pmt\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            break\n",
    "    \n",
    "    if not os.path.exists(logfile):\n",
    "        with open(logfile, \"w\") as f: \n",
    "            msg = ','.join(['epoch']+[f\"train_{k}\" for k in sorted(train_metrics.keys())]+[f\"valid_{k}\" for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "            f.write(msg)\n",
    "            f.flush()\n",
    "            if verbose:\n",
    "                print(msg)\n",
    "\n",
    "    with open(logfile, \"a\") as f: \n",
    "        msg = ','.join([str(epoch)]+[f'{train_metrics[k]:.4f}' for k in sorted(train_metrics.keys())]+[f'{val_metrics[k]:.4f}' for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "        f.write(msg)\n",
    "        f.flush()    \n",
    "        if verbose:\n",
    "            print(msg)\n",
    "        \n",
    "# remove temporal files           \n",
    "shutil.rmtree(config[\"cache_path\"], ignore_errors=True)\n",
    "\n",
    "tmp_file = os.path.join(out_path, \"train.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)\n",
    "tmp_file = os.path.join(out_path, \"valid.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## sincfold_no_ResNet1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ./pruebas/no_ResNet1d/\n",
      "No weights provided, using random initialization\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:02<00:00, 60.80s/it] \n",
      "100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,train_f1,train_loss,valid_f1,valid_f1_post,valid_loss\n",
      "\n",
      "0,0.0010,0.2791,0.0000,0.0000,0.0805\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:46<00:00, 75.58s/it] \n",
      "100%|██████████| 1/1 [00:09<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0.0000,0.2394,0.0000,0.0000,0.0827\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:39<00:00, 73.14s/it] \n",
      "100%|██████████| 1/1 [00:09<00:00,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,0.0000,0.1982,0.0000,0.0000,0.0658\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:12<00:00, 64.01s/it]\n",
      "100%|██████████| 1/1 [00:09<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,0.0200,0.0966,0.0000,0.0000,0.0591\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:23<00:00, 67.78s/it] \n",
      "100%|██████████| 1/1 [00:09<00:00,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,0.1881,0.0715,0.1517,0.1488,0.0779\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:44<00:00, 74.89s/it] \n",
      "100%|██████████| 1/1 [00:08<00:00,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,0.2540,0.0657,0.0930,0.1825,0.0935\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_file ='../data/ArchiveII_sample.csv'\n",
    "\n",
    "valid_file=None\n",
    "out_path='./pruebas/no_ResNet1d/'\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print(\"Working on\", out_path)\n",
    "\n",
    "if \"cache_path\" not in config:\n",
    "    config[\"cache_path\"] = \"cache/\"\n",
    "\n",
    "if not os.path.isdir(out_path):\n",
    "    os.makedirs(out_path)\n",
    "else:\n",
    "    raise ValueError(f\"Output path {out_path} already exists\")\n",
    "\n",
    "if valid_file is not None:\n",
    "    train_file = train_file\n",
    "    valid_file = valid_file\n",
    "else:\n",
    "    data = pd.read_csv(train_file)\n",
    "    valid_split = config[\"valid_split\"] if \"valid_split\" in config else 0.1\n",
    "    train_file = os.path.join(out_path, \"train.csv\")\n",
    "    valid_file = os.path.join(out_path, \"valid.csv\")\n",
    "\n",
    "    val_data = data.sample(frac = valid_split)\n",
    "    val_data.to_csv(valid_file, index=False)\n",
    "    data.drop(val_data.index).to_csv(train_file, index=False)\n",
    "    \n",
    "batch_size = config[\"batch_size\"] if \"batch_size\" in config else 4\n",
    "train_loader = DataLoader(\n",
    "    SeqDataset(train_file, training=True, **config),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    SeqDataset(valid_file, **config),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=nworkers,\n",
    "    collate_fn=pad_batch,\n",
    ")\n",
    "\n",
    "net = sincfold_no_ResNet1d(train_len=len(train_loader), **config)\n",
    "\n",
    "best_f1, patience_counter = -1, 0\n",
    "patience = config[\"patience\"] if \"patience\" in config else 30\n",
    "if verbose:\n",
    "    print(\"Start training...\")\n",
    "max_epochs = config[\"max_epochs\"] if \"max_epochs\" in config else 1000\n",
    "logfile = os.path.join(out_path, \"train_log.csv\") \n",
    "    \n",
    "for epoch in range(6): ## 6 epochs\n",
    "    train_metrics = net.fit(train_loader)\n",
    "\n",
    "    val_metrics = net.test(valid_loader)\n",
    "\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        tr.save(net.state_dict(), os.path.join(out_path, \"weights.pmt\"))\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter > patience:\n",
    "            break\n",
    "    \n",
    "    if not os.path.exists(logfile):\n",
    "        with open(logfile, \"w\") as f: \n",
    "            msg = ','.join(['epoch']+[f\"train_{k}\" for k in sorted(train_metrics.keys())]+[f\"valid_{k}\" for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "            f.write(msg)\n",
    "            f.flush()\n",
    "            if verbose:\n",
    "                print(msg)\n",
    "\n",
    "    with open(logfile, \"a\") as f: \n",
    "        msg = ','.join([str(epoch)]+[f'{train_metrics[k]:.4f}' for k in sorted(train_metrics.keys())]+[f'{val_metrics[k]:.4f}' for k in sorted(val_metrics.keys())]) + \"\\n\"\n",
    "        f.write(msg)\n",
    "        f.flush()    \n",
    "        if verbose:\n",
    "            print(msg)\n",
    "        \n",
    "# remove temporal files           \n",
    "shutil.rmtree(config[\"cache_path\"], ignore_errors=True)\n",
    "\n",
    "tmp_file = os.path.join(out_path, \"train.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)\n",
    "tmp_file = os.path.join(out_path, \"valid.csv\")\n",
    "if os.path.exists(tmp_file):\n",
    "    os.remove(tmp_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sincfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
